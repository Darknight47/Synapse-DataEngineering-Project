{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount ADLS Gen2\n",
    "# We're setting up tiered storage paths in Azure Data Lake Storage Gen2 based on the Medallion Architecture (Bronze, Silver, Gold). \n",
    "# This is a common data lake design pattern used in Azure Synapse and Databricks to organize data efficiently\n",
    "\n",
    "# Bronze Layer: Stores raw, unprocessed data (directly ingested from sources).\n",
    "# Silver Layer: Stores cleaned and validated data (after transformations).\n",
    "# Gold Layer: Stores aggregated and business-ready data (optimized for analytics).\n",
    "\n",
    "tiers = ['bronze', 'silver', 'gold']\n",
    "adls_paths = {tier: f'abfss://{tier}@tut03datalake.dfs.core.windows.net/' for tier in tiers}\n",
    "\n",
    "# tut03datalake\" is the name of your Azure Data Lake Storage Gen2 account.\n",
    "# abfss:// → This is the Azure Blob File System (ABFSS) protocol, used for accessing files in Data Lake Storage Gen2.\n",
    "# {tier} → This is a folder name inside the storage account (e.g., bronze, silver, gold).\n",
    "# tut03datalake → This is your storage account name.\n",
    "# .dfs.core.windows.net → This is the domain for Azure Data Lake Storage Gen2.\n",
    "\n",
    "bronze_adls = adls_paths['bronze']\n",
    "silver_adls = adls_paths['silver']\n",
    "gold_adls = adls_paths['gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = mssparkutils.fs.ls(bronze_adls)\n",
    "for file in files:\n",
    "    print(file.name, file.isDir, file.isFile, file.path, file.size, file.modifyTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308aee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for today and yesterday\n",
    "start_date = date.today() - timedelta(days=1)\n",
    "end_date = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6acc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f125703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the API URL with start and end dates provided by Data Factory\n",
    "url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}\"\n",
    "# This constructs a URL to request earthquake data from the USGS API.\n",
    "# The API returns data in GeoJSON format for the specified date range (start_date to end_date).\n",
    "\n",
    "try:\n",
    "    # Making the API Request\n",
    "    # Make the GET request to fetch data\n",
    "    response = requests.get(url)\n",
    "    # Sends a GET request to the API.\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    response.raise_for_status() # Raise HTTPError for bad response\n",
    "    # raise_for_status() ensures the request was successful—if not, it raises an error.\n",
    "    data = response.json().get('features', [])\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data returned for the specified date range.\")\n",
    "    else:\n",
    "        # Specify the ADLS path\n",
    "        file_path = f\"{bronze_adls}/{start_date}_earthquake_data.json\"\n",
    "\n",
    "        # Convert data to JSON string\n",
    "        json_data = json.dumps(data, indent=4)\n",
    "        # Converts the extracted earthquake data into a formatted JSON string.\n",
    "\n",
    "        # Write the JSON data to ADLS\n",
    "        # Create an RDD with the JSON string and parallelize it\n",
    "        rdd = spark.sparkContext.parallelize([json_data])\n",
    "        # Creates an RDD (Resilient Distributed Dataset) from the JSON string.\n",
    "        # Reads the RDD into a Spark DataFrame.\n",
    "        # Limits the DataFrame to 3 rows (for faster processing in the tutorial).\n",
    "        # Writes the DataFrame to ADLS in JSON format, overwriting any existing file.\n",
    "\n",
    "        # Convert RDD to DataFrame and write to ADLS\n",
    "        df = spark.read.json(rdd)\n",
    "        df.limit(3) # To spead up processing for tutorial\n",
    "        df.write.mode(\"overwrite\").json(file_path)\n",
    "\n",
    "        print(f\"Data successfully saved to {file_path}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data from API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Defining variables\n",
    "output_data = {\n",
    "    \"start_date\" : start_date.isoformat(),\n",
    "    \"bronze_adls\" : bronze_adls,\n",
    "    \"silver_adls\" : silver_adls,\n",
    "    \"gold_adls\" : gold_adls\n",
    "}\n",
    "\n",
    "# Seralizing the dictionary to a JSON format\n",
    "bronze_output = json.dumps(output_data)\n",
    "# Converts the dictionary into a JSON string (bronze_output).\n",
    "# JSON format makes it easy to pass data between services.\n",
    "\n",
    "# Passing the JSON output to the pipeline by using mssparkutils.notebook.exit\n",
    "mssparkutils.notebook.exit(bronze_output)\n",
    "\n",
    "# Allows the pipeline to access notebook results dynamically. \n",
    "# Enables automation—the pipeline can make decisions based on the notebook's output. \n",
    "# Improves data tracking—ensures key variables are available for further processing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
