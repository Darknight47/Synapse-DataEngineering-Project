{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Access parameters directly\n",
    "bronze_output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Try\n",
    "\n",
    "#from datetime import date, timedelta\n",
    "\n",
    "# For getting the data manually without data factory pipeline\n",
    "#start_date = date.today() - timedelta(days=1)\n",
    "\n",
    "#bronze_adls = \"abfss://bronze@tut03datalake.dfs.core.windows.net/\"\n",
    "#silver_adls = \"abfss://silver@tut03datalake.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated process of the above code cell.\n",
    "\n",
    "# Parse the JSON string\n",
    "output_data = json.loads(bronze_output)\n",
    "\n",
    "# Access Individual Variables\n",
    "start_date = output_data.get(\"start_date\")\n",
    "silver_adls = output_data.get(\"silver_adls\")\n",
    "bronze_adls = output_data.get(\"bronze_adls\")\n",
    "\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"Silver ADLS: {silver_adls}\")\n",
    "print(f\"Bronze ADLS: {bronze_adls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnull, when\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data into a Spark Datafrme\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}/{start_date}_earthquake_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790808d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated'),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the data> Checking for missing values \n",
    "df = (\n",
    "    df\n",
    "    .withColumn('longitude', when(isnull(col('longitude')), 0).otherwise(col('longitude')))\n",
    "    .withColumn('latitude', when(isnull(col('latitude')), 0).otherwise(col('latitude')))\n",
    "    .withColumn('time', when(isnull(col('time')), 0).otherwise(col('time')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the time from time=1747438679005, updated=1747454278106 TO\n",
    "# time=datetime.datetime(2025, 5, 16, 23, 37, 59, 5000), updated=datetime.datetime(2025, 5, 17, 3, 57, 58, 106000)\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('time', (col('time') / 1000).cast(TimestampType()))\n",
    "    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08bb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the transformed dataframe to the SILVER container\n",
    "silver_data = f\"{silver_adls}earthquake_event_silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append DF to Silver container in Parquet format\n",
    "df.write.mode('append').parquet(silver_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssparkutils.notebook.exit(silver_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
